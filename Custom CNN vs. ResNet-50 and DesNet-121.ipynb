{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3876fca5",
   "metadata": {},
   "source": [
    "ResNet-50 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98077122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # Truncation for image loading\n",
    "\n",
    "# Image loader\n",
    "def ImageLoader(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        return img.convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping corrupt image: {path}\")\n",
    "        return Image.new('RGB', (224, 224))\n",
    "\n",
    "# Ensure we are using GPU, which is cudo:0 in this case (1st aviable GPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data transforms for augmentation\n",
    "dataTransforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "def loadDatasets():\n",
    "    ds1Path = '/content/chest_xray'\n",
    "    ds2Path = '/content/chest_xray2'\n",
    "\n",
    "    class PneumoniaDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, rootDir, transform=None):\n",
    "            self.rootDir = rootDir\n",
    "            self.transform = transform\n",
    "            self.classes = ['NORMAL', 'PNEUMONIA']\n",
    "            self.classToIdx = {'NORMAL': 0, 'PNEUMONIA': 1}\n",
    "            self.samples = []\n",
    "\n",
    "            for className in self.classes:\n",
    "                classDir = os.path.join(rootDir, className)\n",
    "                if os.path.isdir(classDir):\n",
    "                    for filename in os.listdir(classDir):\n",
    "                        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            path = os.path.join(classDir, filename)\n",
    "                            label = self.classToIdx[className]\n",
    "                            self.samples.append((path, label))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            path, label = self.samples[idx]\n",
    "            img = ImageLoader(path)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label\n",
    "\n",
    "    # Load all of the datasets\n",
    "    ds1Train = PneumoniaDataset(os.path.join(ds1Path, 'train'))\n",
    "    ds2Train = PneumoniaDataset(os.path.join(ds2Path, 'train'))\n",
    "    ds1Test = PneumoniaDataset(os.path.join(ds1Path, 'test'))\n",
    "    ds2Test = PneumoniaDataset(os.path.join(ds2Path, 'test'))\n",
    "\n",
    "    # Combine all samples from both datasets\n",
    "    allSamples = ds1Train.samples + ds2Train.samples + ds1Test.samples + ds2Test.samples\n",
    "    allLabels = [label for _, label in allSamples]\n",
    "\n",
    "    # 70/20/10 split\n",
    "    trainIdx, testValIdx = train_test_split(\n",
    "        range(len(allSamples)),\n",
    "        test_size=0.3,\n",
    "        stratify=allLabels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    testIdx, valIdx = train_test_split(\n",
    "        testValIdx,\n",
    "        test_size=1/3,\n",
    "        stratify=[allLabels[i] for i in testValIdx],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Create base dataset for subsets to manyally contol data entries\n",
    "    baseDataset = PneumoniaDataset('', transform=None)\n",
    "    baseDataset.samples = allSamples\n",
    "\n",
    "    # Subsets with transforms (standard split)\n",
    "    trainDataset = Subset(baseDataset, trainIdx)\n",
    "    valDataset = Subset(baseDataset, valIdx)\n",
    "    testDataset = Subset(baseDataset, testIdx)\n",
    "\n",
    "    trainDataset.dataset.transform = dataTransforms['train']\n",
    "    valDataset.dataset.transform = dataTransforms['val']\n",
    "    testDataset.dataset.transform = dataTransforms['test']\n",
    "\n",
    "    # Class weights\n",
    "    trainLabels = [allLabels[i] for i in trainIdx]\n",
    "    classCounts = np.bincount(trainLabels)\n",
    "    print(f\"Class distribution in training set: {classCounts}\")\n",
    "\n",
    "    weights = 1. / torch.tensor(classCounts, dtype=torch.float32)\n",
    "    sampleWeights = weights[trainLabels]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sampleWeights,\n",
    "        num_samples=len(sampleWeights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    # Print dataset sizes\n",
    "    totalSize = len(allSamples)\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"Train: {len(trainIdx)} ({len(trainIdx)/totalSize:.1%})\")\n",
    "    print(f\"Val: {len(valIdx)} ({len(valIdx)/totalSize:.1%})\")\n",
    "    print(f\"Test: {len(testIdx)} ({len(testIdx)/totalSize:.1%})\")\n",
    "\n",
    "    return trainDataset, valDataset, testDataset, sampler\n",
    "\n",
    "trainDataset, valDataset, testDataset, sampler = loadDatasets()\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(trainDataset, batch_size=32, sampler=sampler, num_workers=4),\n",
    "    'val': DataLoader(valDataset, batch_size=32, shuffle=False, num_workers=4),\n",
    "    'test': DataLoader(testDataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "numFeatures = model.fc.in_features\n",
    "model.fc = nn.Linear(numFeatures, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "trainLabels = [label for _, label in trainDataset]\n",
    "classCounts = np.bincount(trainLabels)\n",
    "classWeights = torch.tensor([1.0/classCounts[0], 1.0/classCounts[1]], dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=classWeights)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Training function w/ mixed precision\n",
    "def trainModel(model, criterion, optimizer, scheduler, numEpochs=25):\n",
    "    since = time.time()\n",
    "    bestModelWeights = copy.deepcopy(model.state_dict())\n",
    "    bestAcc = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        print(f'Epoch {epoch}/{numEpochs-1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            runningLoss = 0.0\n",
    "            runningCorrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                runningLoss += loss.item() * inputs.size(0)\n",
    "                runningCorrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epochLoss = runningLoss / len(dataloaders[phase].dataset)\n",
    "            epochAcc = runningCorrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epochLoss:.4f} Acc: {epochAcc:.4f}')\n",
    "\n",
    "            if phase == 'val' and epochAcc > bestAcc:\n",
    "                bestAcc = epochAcc\n",
    "                bestModelWeights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        scheduler.step()\n",
    "        print()\n",
    "\n",
    "    timeElapsed = time.time() - since\n",
    "    print(f'Training complete in {timeElapsed//60:.0f}m {timeElapsed%60:.0f}s')\n",
    "    print(f'Best val Acc: {bestAcc:.4f}')\n",
    "\n",
    "    model.load_state_dict(bestModelWeights)\n",
    "    return model\n",
    "\n",
    "print(\"Training ResNet-50...\")\n",
    "model = trainModel(model, criterion, optimizer, scheduler, numEpochs=25) # Train model for 25 epochs\n",
    "\n",
    "savePath = \"/content/drive/MyDrive/pneumonia_resnet50.pth\"\n",
    "torch.save(model.state_dict(), savePath)\n",
    "print(f\"Model saved to {savePath}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluateModel(model, dataloader):\n",
    "    model.eval()\n",
    "    allPreds = []\n",
    "    allLabels = []\n",
    "    allProbs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "            allPreds.extend(preds.cpu().numpy())\n",
    "            allLabels.extend(labels.cpu().numpy())\n",
    "            allProbs.extend(probs.cpu().numpy())\n",
    "\n",
    "    return allLabels, allPreds, allProbs\n",
    "\n",
    "testLabels, testPreds, testProbs = evaluateModel(model, dataloaders['test']) # Test evaluation \n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(testLabels, testPreds, target_names=['NORMAL', 'PNEUMONIA']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(testLabels, testPreds))\n",
    "\n",
    "testProbsArray = np.array(testProbs)\n",
    "print(f\"\\nROC AUC: {roc_auc_score(testLabels, testProbsArray[:, 1]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37756f8",
   "metadata": {},
   "source": [
    "DenseNet-121 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Subset\n",
    "from torchvision import models, transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # Truncation for image loading\n",
    "\n",
    "# Image loader\n",
    "def imageLoader(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        return img.convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping corrupt image: {path}\")\n",
    "        return Image.new('RGB', (224, 224))\n",
    "\n",
    "# Ensure we are using GPU, which is cudo:0 in this case (1st aviable GPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data transforms with augmentation\n",
    "dataTransforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "def loadDatasets():\n",
    "    ds1Path = '/content/chest_xray'\n",
    "    ds2Path = '/content/chest_xray2'\n",
    "\n",
    "    class PneumoniaDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, rootDir, transform=None):\n",
    "            self.rootDir = rootDir\n",
    "            self.transform = transform\n",
    "            self.classes = ['NORMAL', 'PNEUMONIA']\n",
    "            self.classToIdx = {'NORMAL': 0, 'PNEUMONIA': 1}\n",
    "            self.samples = []\n",
    "\n",
    "            for className in self.classes:\n",
    "                classDir = os.path.join(rootDir, className)\n",
    "                if os.path.isdir(classDir):\n",
    "                    for filename in os.listdir(classDir):\n",
    "                        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            path = os.path.join(classDir, filename)\n",
    "                            label = self.classToIdx[className]\n",
    "                            self.samples.append((path, label))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            path, label = self.samples[idx]\n",
    "            img = imageLoader(path)\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            return img, label\n",
    "\n",
    "    ds1Train = PneumoniaDataset(os.path.join(ds1Path, 'train'))\n",
    "    ds2Train = PneumoniaDataset(os.path.join(ds2Path, 'train'))\n",
    "    ds1Test = PneumoniaDataset(os.path.join(ds1Path, 'test'))\n",
    "    ds2Test = PneumoniaDataset(os.path.join(ds2Path, 'test'))\n",
    "\n",
    "    allSamples = ds1Train.samples + ds2Train.samples + ds1Test.samples + ds2Test.samples\n",
    "    allLabels = [label for _, label in allSamples]\n",
    "\n",
    "    trainIdx, testValIdx = train_test_split(\n",
    "        range(len(allSamples)),\n",
    "        test_size=0.3,\n",
    "        stratify=allLabels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    testIdx, valIdx = train_test_split(\n",
    "        testValIdx,\n",
    "        test_size=1/3,\n",
    "        stratify=[allLabels[i] for i in testValIdx],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    baseDataset = PneumoniaDataset('', transform=None)\n",
    "    baseDataset.samples = allSamples\n",
    "\n",
    "    trainDataset = Subset(baseDataset, trainIdx)\n",
    "    valDataset = Subset(baseDataset, valIdx)\n",
    "    testDataset = Subset(baseDataset, testIdx)\n",
    "\n",
    "    trainDataset.dataset.transform = dataTransforms['train']\n",
    "    valDataset.dataset.transform = dataTransforms['val']\n",
    "    testDataset.dataset.transform = dataTransforms['test']\n",
    "\n",
    "    trainLabels = [allLabels[i] for i in trainIdx]\n",
    "    classCounts = np.bincount(trainLabels)\n",
    "    print(f\"Class distribution in training set: {classCounts}\")\n",
    "\n",
    "    weights = 1. / torch.tensor(classCounts, dtype=torch.float32)\n",
    "    sampleWeights = weights[trainLabels]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sampleWeights,\n",
    "        num_samples=len(sampleWeights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    totalSize = len(allSamples)\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"Train: {len(trainIdx)} ({len(trainIdx)/totalSize:.1%})\")\n",
    "    print(f\"Val: {len(valIdx)} ({len(valIdx)/totalSize:.1%})\")\n",
    "    print(f\"Test: {len(testIdx)} ({len(testIdx)/totalSize:.1%})\")\n",
    "\n",
    "    return trainDataset, valDataset, testDataset, sampler\n",
    "\n",
    "# Load data\n",
    "trainDataset, valDataset, testDataset, sampler = loadDatasets()\n",
    "\n",
    "# Create dataloaders\n",
    "dataLoaders = {\n",
    "    'train': DataLoader(trainDataset, batch_size=32, sampler=sampler, num_workers=4),\n",
    "    'val': DataLoader(valDataset, batch_size=32, shuffle=False, num_workers=4),\n",
    "    'test': DataLoader(testDataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "}\n",
    "\n",
    "# Initialize DenseNet-121 model\n",
    "model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "\n",
    "# Replaec original classifer with linear layer that outputs binary classification\n",
    "numFeatures = model.classifier.in_features\n",
    "model.classifier = nn.Linear(numFeatures, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function with class weighting\n",
    "trainLabels = [label for _, label in trainDataset]\n",
    "classCounts = np.bincount(trainLabels)\n",
    "classWeights = torch.tensor([1.0/classCounts[0], 1.0/classCounts[1]], dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=classWeights)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Training function\n",
    "def trainModel(model, criterion, optimizer, scheduler, numEpochs=25):\n",
    "    since = time.time()\n",
    "    bestModelWeights = copy.deepcopy(model.state_dict())\n",
    "    bestAcc = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        print(f'Epoch {epoch}/{numEpochs-1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            runningLoss = 0.0\n",
    "            runningCorrects = 0\n",
    "\n",
    "            for inputs, labels in dataLoaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                runningLoss += loss.item() * inputs.size(0)\n",
    "                runningCorrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epochLoss = runningLoss / len(dataLoaders[phase].dataset)\n",
    "            epochAcc = runningCorrects.double() / len(dataLoaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epochLoss:.4f} Acc: {epochAcc:.4f}')\n",
    "\n",
    "            if phase == 'val' and epochAcc > bestAcc:\n",
    "                bestAcc = epochAcc\n",
    "                bestModelWeights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        scheduler.step()\n",
    "        print()\n",
    "\n",
    "    timeElapsed = time.time() - since\n",
    "    print(f'Training complete in {timeElapsed//60:.0f}m {timeElapsed%60:.0f}s')\n",
    "    print(f'Best val Acc: {bestAcc:.4f}')\n",
    "\n",
    "    model.load_state_dict(bestModelWeights)\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Training DenseNet-121...\")\n",
    "model = trainModel(model, criterion, optimizer, scheduler, numEpochs=25) # Train model\n",
    "\n",
    "# Save model\n",
    "savePath = \"/content/drive/MyDrive/pneumonia_densenet121.pth\"\n",
    "torch.save(model.state_dict(), savePath)\n",
    "print(f\"Model saved to {savePath}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluateModel(model, dataLoader):\n",
    "    model.eval()\n",
    "    allPreds = []\n",
    "    allLabels = []\n",
    "    allProbs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataLoader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "            allPreds.extend(preds.cpu().numpy())\n",
    "            allLabels.extend(labels.cpu().numpy())\n",
    "            allProbs.extend(probs.cpu().numpy())\n",
    "\n",
    "    return allLabels, allPreds, allProbs\n",
    "\n",
    "# Test evaluations\n",
    "testLabels, testPreds, testProbs = evaluateModel(model, dataLoaders['test'])\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(testLabels, testPreds, target_names=['NORMAL', 'PNEUMONIA']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(testLabels, testPreds))\n",
    "\n",
    "testProbsArray = np.array(testProbs)\n",
    "print(f\"\\nROC AUC: {roc_auc_score(testLabels, testProbsArray[:, 1]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915530d6",
   "metadata": {},
   "source": [
    "PneumoniaNet (Custom CNN Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de963de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader, WeightedRandomSampler, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # Truncation for image loading\n",
    "\n",
    "# Image loader\n",
    "def imageLoader(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        return img.convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping corrupt image: {path}\")\n",
    "        return Image.new('RGB', (224, 224))  \n",
    "\n",
    "# Ensure we are using GPU, which is cudo:0 in this case (1st aviable GPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data transforms with augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),  # rotation\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # color jitter\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # small translations\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "def load_datasets():\n",
    "    ds1_path = '/content/chest_xray'\n",
    "    ds2_path = '/content/chest_xray2'\n",
    "\n",
    "    # Custom dataset class for the data\n",
    "    class PneumoniaDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, root_dir, transform=None, loader=imageLoader):\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "            self.loader = loader\n",
    "            self.classes = ['NORMAL', 'PNEUMONIA']  # Match classification names\n",
    "            self.class_to_idx = {'NORMAL': 0, 'PNEUMONIA': 1}\n",
    "\n",
    "            # Find all image paths and their labels\n",
    "            self.samples = []\n",
    "            for class_name in self.classes:\n",
    "                class_dir = os.path.join(root_dir, class_name)\n",
    "                if os.path.isdir(class_dir):\n",
    "                    for filename in os.listdir(class_dir):\n",
    "                        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            path = os.path.join(class_dir, filename)\n",
    "                            label = self.class_to_idx[class_name]\n",
    "                            self.samples.append((path, label))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            path, label = self.samples[idx]\n",
    "            img = self.loader(path)\n",
    "\n",
    "            # Filter out blank images (corrupted or empty imagges)\n",
    "            if img is not None and all(np.array(img).flatten() == 0):\n",
    "                img = Image.new('RGB', (224, 224)) # default placeholder if image is not corrupted\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            return img.float(), label  # Return as float32\n",
    "\n",
    "    # Load all datasets from both sources (without transforms for now)\n",
    "    ds1_train_set = PneumoniaDataset(os.path.join(ds1_path, 'train'), transform=None)\n",
    "    ds2_train_set = PneumoniaDataset(os.path.join(ds2_path, 'train'), transform=None)\n",
    "    ds1_test_set = PneumoniaDataset(os.path.join(ds1_path, 'test'), transform=None)\n",
    "    ds2_test_set = PneumoniaDataset(os.path.join(ds2_path, 'test'), transform=None)\n",
    "\n",
    "    # Combine all samples\n",
    "    all_samples = ds1_train_set.samples + ds2_train_set.samples + ds1_test_set.samples + ds2_test_set.samples\n",
    "    all_labels = [label for _, label in all_samples]\n",
    "\n",
    "    # Create indices for for the 70/20/10 split\n",
    "    indices = list(range(len(all_samples)))\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        indices,\n",
    "        test_size=0.3,  # 30% for test+validation\n",
    "        stratify=all_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Split the remaining 30% into 20% test and 10% validation\n",
    "    test_idx, val_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        test_size=0.33,  # 10% of total (1/3 of 30%)\n",
    "        stratify=[all_labels[i] for i in temp_idx],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Create dataset splits with appropriate transforms\n",
    "    class SubsetDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, samples, transform=None, loader=imageLoader):\n",
    "            self.samples = samples\n",
    "            self.transform = transform\n",
    "            self.loader = loader\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            path, label = self.samples[idx]\n",
    "            img = self.loader(path)\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            return img.float(), label  # Ensure float32 output\n",
    "\n",
    "    # Create subsets with proper transforms\n",
    "    train_samples = [all_samples[i] for i in train_idx]\n",
    "    val_samples = [all_samples[i] for i in val_idx]\n",
    "    test_samples = [all_samples[i] for i in test_idx]\n",
    "\n",
    "    train_dataset = SubsetDataset(train_samples, transform=data_transforms['train'])\n",
    "    val_dataset = SubsetDataset(val_samples, transform=data_transforms['val'])\n",
    "    test_dataset = SubsetDataset(test_samples, transform=data_transforms['val'])\n",
    "\n",
    "    # Extract train labels for computing class weights and sampler\n",
    "    train_labels = [label for _, label in train_samples]\n",
    "    class_sample_counts = np.bincount(train_labels)\n",
    "    print(f\"Class distribution in training set: {class_sample_counts}\")\n",
    "\n",
    "    # Create weighted sampler for balanced training\n",
    "    weights = 1. / torch.tensor(class_sample_counts, dtype=torch.float32)  # float32\n",
    "    samples_weights = weights[train_labels]\n",
    "    sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
    "\n",
    "    # Display dataset sizes\n",
    "    total_size = len(train_samples) + len(val_samples) + len(test_samples)\n",
    "    print(f\"\\nDataset split sizes:\")\n",
    "    print(f\"- Training: {len(train_samples)} images ({len(train_samples)/total_size*100:.1f}%)\")\n",
    "    print(f\"- Validation: {len(val_samples)} images ({len(val_samples)/total_size*100:.1f}%)\")\n",
    "    print(f\"- Test: {len(test_samples)} images ({len(test_samples)/total_size*100:.1f}%)\")\n",
    "    print(f\"- Total: {total_size} images\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, sampler\n",
    "\n",
    "# Create datasets with new split\n",
    "train_dataset, val_dataset, test_dataset, sampler = load_datasets()\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=16, sampler=sampler, num_workers=2, pin_memory=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True),\n",
    "    'test': DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
    "}\n",
    "\n",
    "# Calculate dataset sizes\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'val': len(val_dataset),\n",
    "    'test': len(test_dataset)\n",
    "}\n",
    "\n",
    "# Updated model with further regularization\n",
    "class PneumoniaNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(PneumoniaNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.1),  # spatial dropout\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.1),  # spatial dropout\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.2),  # spatial dropout\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x.float()  # Ensure float32 output\n",
    "\n",
    "# Using transfer learning model with ResNet18\n",
    "def get_transfer_model(num_classes=2):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # Freeze early layers. Do this because when we freeze a large portion of the model, \n",
    "    # we update fewer weights during backpropagation, reducing memory useage and speeds up trainging.\n",
    "    for param in list(model.parameters())[:-8]:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace final fully connected layer\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(num_ftrs, num_classes)\n",
    "    )\n",
    "\n",
    "    return model.float()  # Ensure float32 output\n",
    "\n",
    "use_transfer_learning = True\n",
    "print(f\"Using {'transfer learning model (ResNet18)' if use_transfer_learning else 'custom PneumoniaNet model'}\")\n",
    "\n",
    "# Initialize model\n",
    "if use_transfer_learning:\n",
    "    model = get_transfer_model().to(device)\n",
    "else:\n",
    "    model = PneumoniaNet().to(device)\n",
    "\n",
    "# Calculate class weights based on training data\n",
    "train_labels = [y for _, y in train_dataset.samples]\n",
    "class_counts = np.bincount(train_labels)\n",
    "total = sum(class_counts)\n",
    "class_weights = torch.tensor([total/class_counts[0], total/class_counts[1]], dtype=torch.float32).to(device)  # float32\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer with weight decay for L2 regularization\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# Improved learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Enhanced training function with early stopping\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, patience=7):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    # For tracking metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        # Gradient clipping to prevent exploding gradients\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                train_accs.append(epoch_acc.item())\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "                val_accs.append(epoch_acc.item())\n",
    "\n",
    "                # Update scheduler based on validation loss\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            # Deep copy the model if best validation accuracy\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    no_improve_epochs = 0\n",
    "                    # Save best model\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'accuracy': best_acc,\n",
    "                    }, '/content/drive/MyDrive/pneumonia_best_model.pth')\n",
    "                    print(f'New best model saved with accuracy: {best_acc:.4f}')\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "\n",
    "        # Keeping track of epochs without improvement, but not stopping\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f'No improvement for {patience} epochs, but continuing training until end at least 30 per document requirement of at least 25')\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Return metrics for reporting/visualization\n",
    "    metrics = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "\n",
    "    return model, metrics\n",
    "\n",
    "# Train the model with early stopping\n",
    "print(\"Training Pneumonia Classification Model...\")\n",
    "model, metrics = train_model(model, criterion, optimizer, scheduler, num_epochs=30, patience=7)\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'metrics': metrics,\n",
    "}, '/content/drive/MyDrive/pneumonia_final_model.pth')\n",
    "\n",
    "print(\"Training completed. Model saved!\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, criterion, dataset_name='test'):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # No gradients needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders[dataset_name]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # confusion matrix\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    eval_loss = running_loss / dataset_sizes[dataset_name]\n",
    "    eval_acc = running_corrects.double() / dataset_sizes[dataset_name]\n",
    "\n",
    "    print(f'\\nFinal Evaluation on {dataset_name} set:')\n",
    "    print(f'{dataset_name.capitalize()} Loss: {eval_loss:.4f}')\n",
    "    print(f'{dataset_name.capitalize()} Accuracy: {eval_acc:.4f}')\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Run final evaluation on test set\n",
    "print(\"\\nPerforming final evaluation on test set...\")\n",
    "predictions, true_labels = evaluate_model(model, criterion, 'test')\n",
    "\n",
    "# Compute confusion matrix, precision, recall, etc.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=['Normal', 'Pneumonia']))\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
